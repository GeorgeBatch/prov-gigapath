{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prov-GigaPath Demo\n",
    "\n",
    "This notebook provides a quick walkthrough of the Prov-GigaPath models. We will start by demonstrating how to download the Prov-GigaPath models from HuggingFace. Next, we will show an example of pre-processing a slide. Finally, we will demonstrate how to run Prov-GigaPath on the sample slide.\n",
    "\n",
    "### Prepare HF Token\n",
    "\n",
    "To begin, please request access to the model from our HuggingFace repository: https://huggingface.co/prov-gigapath/prov-gigapath.\n",
    "\n",
    "Once approved, set the HF_TOKEN to access the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Please set your Hugging Face API token\n",
    "# os.environ[\"HF_TOKEN\"] = \"YOUR_HF_TOKEN\"\n",
    "\n",
    "homedir_path = os.path.expanduser(\"~\")\n",
    "assert (\"HF_TOKEN\" in os.environ) or os.path.exists(f\"{homedir_path}/.cache/huggingface/token\"), \"Please set the HF_TOKEN environment variable to your Hugging Face API token or make sure the token is cached in ~/.cache/huggingface/token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = \"..\"\n",
    "\n",
    "local_dir_name = \"sample_data\"\n",
    "local_dir = os.path.join(PROJECT_DIR, local_dir_name)\n",
    "\n",
    "slide_file_name = \"PROV-000-000001.ndpi\"\n",
    "slide_hf_path = os.path.join(local_dir_name, slide_file_name)\n",
    "slide_path = os.path.join(local_dir, \"PROV-000-000001.ndpi\")\n",
    "\n",
    "\n",
    "tile_save_dir = os.path.join(local_dir, \"outputs/preprocessing\")\n",
    "specific_slide_tiles_dir = f\"{tile_save_dir}/output/{slide_file_name}\"\n",
    "os.makedirs(specific_slide_tiles_dir, exist_ok=True)\n",
    "\n",
    "features_save_dir = os.path.join(local_dir, \"outputs/features\")\n",
    "specific_slide_features_dir = f\"{features_save_dir}/output/{slide_file_name}\"\n",
    "os.makedirs(specific_slide_features_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a sample slide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "huggingface_hub.hf_hub_download(\n",
    "    \"prov-gigapath/prov-gigapath\",\n",
    "    filename=slide_hf_path,\n",
    "    local_dir=PROJECT_DIR,\n",
    "    force_download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiling\n",
    "\n",
    "Whole-slide images are giga-pixel in size. To efficiently process these enormous images, we use a tiling technique that divides them into smaller, more manageable tile images. As an example, we demonstrate how to process a single slide.\n",
    "\n",
    "NOTE: Prov-GigaPath is trained with slides preprocessed at 0.5 MPP. Ensure that you use the appropriate level for the 0.5 MPP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gigapath.pipeline import tile_one_slide\n",
    "\n",
    "print(\"NOTE: Prov-GigaPath is trained with 0.5 mpp preprocessed slides. Please make sure to use the appropriate level for the 0.5 MPP\")\n",
    "tile_one_slide(slide_path, save_dir=tile_save_dir, level=1) # tile_size=256 is the default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the tile images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [\n",
    "    os.path.join(specific_slide_tiles_dir, img)\n",
    "    for img in os.listdir(specific_slide_tiles_dir)\n",
    "    if img.endswith(\".png\")\n",
    "]\n",
    "\n",
    "print(f\"Found {len(image_paths)} image tiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt to match extracted tile and its real coordinates - Failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_paths = sorted(image_paths)\n",
    "tile_file_names = [\n",
    "    os.path.basename(sample_tile_path) for sample_tile_path in tile_paths\n",
    "]\n",
    "coordinates = [\n",
    "    tuple(\n",
    "        int(coord.replace(\"x\", \"\").replace(\"y\", \"\"))\n",
    "        for coord in os.path.basename(sample_tile_path).split(\".png\")[0].split(\"_\")\n",
    "    )\n",
    "    for sample_tile_path in tile_paths\n",
    "]\n",
    "# choose the coordinates pair with the largest x, given larges x, choose the largest y\n",
    "# do it in 2 steps to avoid sorting the coordinates\n",
    "max_x = max(coordinates, key=lambda x: x[0])[0]\n",
    "max_x_coordinates = [coord for coord in coordinates if coord[0] == max_x]\n",
    "max_y = max(max_x_coordinates, key=lambda x: x[1])[1]\n",
    "\n",
    "max_tile_file_name = f\"{max_x}x_{max_y}y.png\"\n",
    "assert (\n",
    "    max_tile_file_name in tile_file_names\n",
    "), f\"Missing tile at coordinates ({max_x}, {max_y})\"\n",
    "\n",
    "print(max_tile_file_name)\n",
    "plt.imshow(plt.imread(os.path.join(specific_slide_tiles_dir, max_tile_file_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openslide\n",
    "\n",
    "# maybe we need to offset the coordinates by the min_x, min_y\n",
    "min_x = min(coordinates, key=lambda x: x[0])[0]\n",
    "min_y = min(coordinates, key=lambda x: x[1])[1]\n",
    "print(f\"Min coordinates: ({min_x}, {min_y})\")\n",
    "\n",
    "# does no match the max_x, max_y tile extracted above\n",
    "\n",
    "sample_slide = openslide.OpenSlide(slide_path)\n",
    "print(help(sample_slide.read_region))\n",
    "sample_slide.read_region((min_x, min_y), 5, (256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Prov-GigaPath model (tile and slide encoder models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_encoder = timm.create_model(\"hf_hub:prov-gigapath/prov-gigapath\", pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gigapath.pipeline import load_tile_slide_encoder\n",
    "\n",
    "# Load the tile and slide encoder models\n",
    "# NOTE: The CLS token is not trained during the slide-level pretraining.\n",
    "# Here, we enable the use of global pooling for the output embeddings.\n",
    "\n",
    "# tile_encoder, slide_encoder_model = load_tile_slide_encoder(global_pool=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run tile-level inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gigapath.pipeline import run_inference_with_tile_encoder\n",
    "\n",
    "tile_encoder_outputs = run_inference_with_tile_encoder(image_paths, tile_encoder, batch_size=32)\n",
    "\n",
    "for k in tile_encoder_outputs.keys():\n",
    "    print(f\"tile_encoder_outputs[{k}].shape: {tile_encoder_outputs[k].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features and coordinates pytorch tensors\n",
    "torch.save(tile_encoder_outputs[\"tile_embeds\"], os.path.join(specific_slide_features_dir, \"tile_embeds.pt\"),)\n",
    "torch.save(tile_encoder_outputs[\"coords\"], os.path.join(specific_slide_features_dir, \"coords.pt\"),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up GPU memory\n",
    "del tile_encoder\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run slide-level inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_encoder_outputs = {}\n",
    "tile_encoder_outputs[\"tile_embeds\"] = torch.load(os.path.join(specific_slide_features_dir, \"tile_embeds.pt\"))\n",
    "tile_encoder_outputs[\"coords\"] = torch.load(os.path.join(specific_slide_features_dir, \"coords.pt\"))\n",
    "\n",
    "tile_encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gigapath.slide_encoder as slide_encoder\n",
    "\n",
    "# load from the web\n",
    "slide_encoder_model = slide_encoder.create_model(\n",
    "    \"hf_hub:prov-gigapath/prov-gigapath\",\n",
    "    \"gigapath_slide_enc12l768d\",\n",
    "    1536,\n",
    "    global_pool=True,  # like in the demo cell above\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gigapath.pipeline import run_inference_with_slide_encoder\n",
    "# run inference with the slide encoder\n",
    "slide_embeds = run_inference_with_slide_encoder(slide_encoder_model=slide_encoder_model, **tile_encoder_outputs)\n",
    "\n",
    "for k in slide_embeds.keys():\n",
    "    print(f\"slide_embeds[{k}].shape: {slide_embeds[k].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_embeds[\"last_layer_embed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gigapath.slide_encoder function `coords_to_pos` - this is not needed to make it work, but it is useful to understand how the positional embeddings are calculated\n",
    "\n",
    "slide_ngrids = 1000\n",
    "\n",
    "coords_ = torch.floor(tile_encoder_outputs[\"coords\"] / 256.0)\n",
    "print(coords_)\n",
    "print(\"/n coords_.min(axis=0)\", coords_.min(axis=0))\n",
    "print(\"/n coords_.max(axis=0)\", coords_.max(axis=0))\n",
    "\n",
    "\n",
    "# pos = coords_[..., 0] * self.slide_ngrids + coords_[..., 1]\n",
    "pos = coords_[..., 0] * slide_ngrids + coords_[..., 1]\n",
    "\n",
    "# return pos.long() + 1  # add 1 for the cls token\n",
    "pos.long() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gigapath",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
