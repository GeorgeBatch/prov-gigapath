{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prov-GigaPath Demo\n",
    "\n",
    "This notebook provides a quick walkthrough of the Prov-GigaPath models. We will start by demonstrating how to download the Prov-GigaPath models from HuggingFace. Next, we will show an example of pre-processing a slide. Finally, we will demonstrate how to run Prov-GigaPath on the sample slide.\n",
    "\n",
    "### Prepare HF Token\n",
    "\n",
    "To begin, please request access to the model from our HuggingFace repository: https://huggingface.co/prov-gigapath/prov-gigapath.\n",
    "\n",
    "Once approved, set the HF_TOKEN to access the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Please set your Hugging Face API token\n",
    "# os.environ[\"HF_TOKEN\"] = \"YOUR_HF_TOKEN\"\n",
    "\n",
    "homedir_path = os.path.expanduser(\"~\")\n",
    "assert (\"HF_TOKEN\" in os.environ) or os.path.exists(f\"{homedir_path}/.cache/huggingface/token\"), \"Please set the HF_TOKEN environment variable to your Hugging Face API token or make sure the token is cached in ~/.cache/huggingface/token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = \"..\"\n",
    "\n",
    "local_dir_name = \"sample_data\"\n",
    "local_dir = os.path.join(PROJECT_DIR, local_dir_name)\n",
    "\n",
    "slide_file_name = \"PROV-000-000001.ndpi\"\n",
    "slide_hf_path = os.path.join(local_dir_name, slide_file_name)\n",
    "slide_path = os.path.join(local_dir, \"PROV-000-000001.ndpi\")\n",
    "\n",
    "\n",
    "tile_save_dir = os.path.join(local_dir, \"outputs/preprocessing\")\n",
    "specific_slide_tiles_dir = f\"{tile_save_dir}/output/{slide_file_name}\"\n",
    "os.makedirs(specific_slide_tiles_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a sample slide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PROV-000-000001.ndpi: 100%|██████████| 424M/424M [00:30<00:00, 14.0MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../sample_data/PROV-000-000001.ndpi'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "huggingface_hub.hf_hub_download(\n",
    "    \"prov-gigapath/prov-gigapath\",\n",
    "    filename=slide_hf_path,\n",
    "    local_dir=PROJECT_DIR,\n",
    "    force_download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiling\n",
    "\n",
    "Whole-slide images are giga-pixel in size. To efficiently process these enormous images, we use a tiling technique that divides them into smaller, more manageable tile images. As an example, we demonstrate how to process a single slide.\n",
    "\n",
    "NOTE: Prov-GigaPath is trained with slides preprocessed at 0.5 MPP. Ensure that you use the appropriate level for the 0.5 MPP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Prov-GigaPath is trained with 0.5 mpp preprocessed slides. Please make sure to use the appropriate level for the 0.5 MPP\n",
      "Warning: Directory ../sample_data/outputs/preprocessing already exists. \n",
      "Processing slide ../sample_data/PROV-000-000001.ndpi at level 1 with tile size 256. Saving to ../sample_data/outputs/preprocessing.\n",
      "('slide_id', 'tile_id', 'image', 'label', 'tile_x', 'tile_y', 'occupancy')\n",
      "Slide ../sample_data/PROV-000-000001.ndpi has been tiled. 1068 tiles saved to ../sample_data/outputs/preprocessing/output/PROV-000-000001.ndpi.\n"
     ]
    }
   ],
   "source": [
    "from gigapath.pipeline import tile_one_slide\n",
    "\n",
    "print(\"NOTE: Prov-GigaPath is trained with 0.5 mpp preprocessed slides. Please make sure to use the appropriate level for the 0.5 MPP\")\n",
    "tile_one_slide(slide_path, save_dir=tile_save_dir, level=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the tile images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1068 image tiles\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# load image tiles\n",
    "image_paths = [\n",
    "    os.path.join(specific_slide_tiles_dir, img)\n",
    "    for img in os.listdir(specific_slide_tiles_dir)\n",
    "    if img.endswith(\".png\")\n",
    "]\n",
    "\n",
    "print(f\"Found {len(image_paths)} image tiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Prov-GigaPath model (tile and slide encoder models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tile encoder param # 1134953984\n",
      "dilated_ratio:  [1, 2, 4, 8, 16]\n",
      "segment_length:  [1024, 5792, 32768, 185363, 1048576]\n",
      "Number of trainable LongNet parameters:  85148160\n",
      "Global Pooling: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "slide_encoder.pth: 100%|██████████| 345M/345M [00:01<00:00, 235MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m Successfully Loaded Pretrained GigaPath model from hf_hub:prov-gigapath/prov-gigapath \u001b[00m\n",
      "Slide encoder param # 86330880\n"
     ]
    }
   ],
   "source": [
    "from gigapath.pipeline import load_tile_slide_encoder\n",
    "\n",
    "# Load the tile and slide encoder models\n",
    "# NOTE: The CLS token is not trained during the slide-level pretraining.\n",
    "# Here, we enable the use of global pooling for the output embeddings.\n",
    "tile_encoder, slide_encoder_model = load_tile_slide_encoder(global_pool=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run tile-level inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference with tile encoder: 100%|██████████| 9/9 [00:09<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tile_encoder_outputs[tile_embeds].shape: torch.Size([1068, 1536])\n",
      "tile_encoder_outputs[coords].shape: torch.Size([1068, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from gigapath.pipeline import run_inference_with_tile_encoder\n",
    "\n",
    "tile_encoder_outputs = run_inference_with_tile_encoder(image_paths, tile_encoder)\n",
    "\n",
    "for k in tile_encoder_outputs.keys():\n",
    "    print(f\"tile_encoder_outputs[{k}].shape: {tile_encoder_outputs[k].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run slide-level inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['layer_0_embed', 'layer_1_embed', 'layer_2_embed', 'layer_3_embed', 'layer_4_embed', 'layer_5_embed', 'layer_6_embed', 'layer_7_embed', 'layer_8_embed', 'layer_9_embed', 'layer_10_embed', 'layer_11_embed', 'layer_12_embed', 'last_layer_embed'])\n"
     ]
    }
   ],
   "source": [
    "from gigapath.pipeline import run_inference_with_slide_encoder\n",
    "# run inference with the slide encoder\n",
    "slide_embeds = run_inference_with_slide_encoder(slide_encoder_model=slide_encoder_model, **tile_encoder_outputs)\n",
    "print(slide_embeds.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gigapath",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
